{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddffbf3a-2dba-4aae-88b1-897a85318e1f",
   "metadata": {},
   "source": [
    "<h2>FILTERING DATA FOR THE GUESSING GAME</h2>\n",
    "    \n",
    "<p>Author: Saffanah Fathin\n",
    "<p>Python Version 3.10\n",
    "<p>Encoding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f20052-bddd-4afe-aead-bb38afdf09cf",
   "metadata": {},
   "source": [
    "Summary: This script of python codes in the following are about the process of filtering the corpus data for the sentence guessing game. With the scripts, there are two txt files generated. One file under the name of \"dataforgame.txt\" will be the main data for the game and another file of \"data_words_text.txt\" is used as the basis for the Hint 3 in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f6071-3fb7-4b2d-b7b9-b7f19b749d6b",
   "metadata": {},
   "source": [
    "Step 1: Importing the corpus data in the format of Collu by using NLTK and store it into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42ee3a1-b5a5-4b3b-83de-29cf52edfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignoring all the collumns except the sentences and POS\n",
    "from nltk.corpus.reader.conll import ConllCorpusReader\n",
    "data = ConllCorpusReader('./', ['en_ewt-ud-train_preproc.conllu'], \n",
    "                           ['ignore', 'words', 'ignore', 'pos', 'ignore', 'ignore', 'ignore', 'ignore', 'ignore', 'ignore'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a19d7-4da9-4cb8-9126-b3a2b8181222",
   "metadata": {},
   "source": [
    "Step 2: Storing Necessary variables for the filtering process (sentences, pos, rare words, and sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930d4e97-b130-4acf-bcb4-d4afcab249ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords in English\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202dc249-05e6-4423-893f-edc83baefd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing necessary data into variables\n",
    "words_list = data.words() #a list of words\n",
    "sent_list = data.sents() #a list of sentences\n",
    "\n",
    "#Part of Speech \n",
    "pos_words = data.tagged_words()\n",
    "pos_sent = data.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54b3f0f-387f-451d-83d0-f4755cbcea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the library\n",
    "from nltk import FreqDist\n",
    "\n",
    "#storing all the frequency of words in the corpus data\n",
    "freqs = FreqDist(data.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851d7962-96ef-4809-a4a7-17e8d51fa57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words that occur less than 10 times\n",
    "rare_words = [word for word, freq in freqs.items() if freq <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d2a466-df5b-4342-80d8-0dbac197ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of POS Tags:\n",
    "pos_freqs = FreqDist([pos for (word, pos) in pos_words])\n",
    "\n",
    "#Sentence lengths\n",
    "sent_lens = [len(s) for s in sent_list]\n",
    "len_freqs = FreqDist(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edb295-7da4-4ae8-b922-a715671bb306",
   "metadata": {},
   "source": [
    " <h3> Filtering the data: </h3>\n",
    "    1. Eliminating sentences that are too long and too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa08266c-7461-4727-a1c0-3828e2d1a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminating too long and too short sentences from the data\n",
    "datacut = [i for i in sent_list if 5<len(i)<20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ebd93-04f8-4a75-9486-1dae8b8345d4",
   "metadata": {},
   "source": [
    "2. Eliminating sentences that contain rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3ed90a-214e-42c4-a819-61b32cf67b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing sentences that contain rare words\n",
    "data_no_rare = [sublist for sublist in datacut if not any(x in sublist for x in rare_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b42690-e7f3-49dc-b3cc-6cb50d690595",
   "metadata": {},
   "source": [
    "   3. Removing punctuations from the data\n",
    "   \n",
    "Note: I decide to remove each punctuations from the data (minus apostrophe for contraction words) for the simplicity of the data in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd45b8b5-513f-48f5-bdd8-e151f6569c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the punctuations\n",
    "from string import punctuation\n",
    "#defining function to split string into characters\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "#convert punctuation into a list\n",
    "punc = (split(punctuation))\n",
    "\n",
    "#eliminating punctuation from the dataset\n",
    "data_no_punc = [[w for w in data if w not in punc] for data in data_no_rare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f6f9caa-18cb-4346-b2fc-09f271207957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert every words into lowercase\n",
    "dataclean = [[w.lower() for w in sublist] for sublist in data_no_punc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe21c80-a11e-442d-a6d2-9af488463076",
   "metadata": {},
   "source": [
    "Note: When I check the data manually, I realized there are punctuations that occurs more than one time, so I decide to also remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b6a964-4ff6-4fed-8d31-7b5497efdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#replacing some punctuations from words list in the data to empty string\n",
    "punkt = '!-?.,:)'\n",
    "new_dat = [[''.join(letter for letter in word if letter not in punkt) for word in sent if word]for sent in dataclean]\n",
    "\n",
    "#removing those empty strings from the list\n",
    "new_dataclean = [[word.translate(string.punctuation) for word in sent if word] for sent in new_dat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeb80c-f85e-49a8-b3a3-0f8f5e2d8625",
   "metadata": {},
   "source": [
    "    5. Further filtering for unsuitable sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ab3317-4b7c-456d-9906-bced8ba9ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#special case: delete sentences that starts with the word 'Carol'\n",
    "carol = [\"carol\"]\n",
    "new_dataclean_nocarol = [sublist for sublist in new_dataclean if not any(x in sublist for x in carol)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e0500-0e0a-45fe-8004-49baf88ce13e",
   "metadata": {},
   "source": [
    "<h3> Storing all necessary data into txt file for the purpose of accesssibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487d3028-41b4-440e-a7e6-ed84a929d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the final data consisting of sentences for the game in the txt file\n",
    "with open(\"dataforgame.txt\", 'w') as f:\n",
    "        for item in new_dataclean_nocarol:\n",
    "            s = \" \".join(map(str, item))\n",
    "            f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ddcd4e-dde5-47b9-8078-5a7b61ff8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the words list of the corpus into a txt file for the purpose of hint number 3 in main.py\n",
    "with open(\"data_words_text.txt\", 'w', encoding='UTF-8') as f:\n",
    "    for item in words_list:\n",
    "            s = str(item + \" \")\n",
    "            f.write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
